{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle-challenge-05.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "LMf7XJ8ZKvMI"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Virtual Environment",
      "language": "python",
      "name": "virtualenvironment"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/kabrol98/PASSNYC-Data-Challenge/blob/master/kaggle_challenge_05.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "LMf7XJ8ZKvMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "kQMLzJ8PMZKN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm going to import all libraries and upload all necessary files here.\n",
        "\n",
        "I will be uploading into colab. Setting up a virtual environment for myself is not an andeavor I'm interested in wasting this weekend on."
      ]
    },
    {
      "metadata": {
        "id": "Mp4l6P6sKvMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52d91613-676f-40ef-9ea6-9ed19679a608"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.models import load_model\n",
        "import io\n",
        "from google.colab import files\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rHTnX2_tSJmW",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "4d2f89bf-5203-4b36-99b0-39a2fe3a2eb8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "school_data = pd.read_csv(io.StringIO(uploaded['2016 School Explorer.csv'].decode('utf-8')))\n",
        "# turnout_data = pd.read_csv(io.StringIO(uploaded['D5 SHSAT Registrations and Testers.csv'].decode('utf-8')))\n",
        "nytdf = pd.read_csv(io.StringIO(uploaded['nytdf.csv'].decode('utf-8')))\n",
        "middleschoolDirectory = pd.read_csv(io.StringIO(uploaded[\"2018_DOE_Middle_School_Directory.csv\"].decode('utf-8')))\n",
        "middleschoolAttendance = pd.read_csv(io.StringIO(uploaded[\"2016-2017_Monthly_Attendance.csv\"].decode('utf-8')))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-860d58c5-add4-4dbd-a018-446c4defb1a1\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-860d58c5-add4-4dbd-a018-446c4defb1a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 2016 School Explorer.csv to 2016 School Explorer.csv\n",
            "Saving 2016-2017_Monthly_Attendance.csv to 2016-2017_Monthly_Attendance.csv\n",
            "Saving 2018_DOE_Middle_School_Directory.csv to 2018_DOE_Middle_School_Directory.csv\n",
            "Saving D5 SHSAT Registrations and Testers.csv to D5 SHSAT Registrations and Testers.csv\n",
            "Saving nytdf.csv to nytdf.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ifU1CkS5BuDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "649d927c-0320-487d-bd07-6632849ebb16"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "middleschoolAttendance.head()\n",
        "\n",
        "year_8_attendance = middleschoolAttendance.query('GradeLevel == \\'08\\'')\n",
        "\n",
        "year_8_attendance.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>School</th>\n",
              "      <th>MonthCode</th>\n",
              "      <th>CalMonth</th>\n",
              "      <th>GradeLevel</th>\n",
              "      <th>GradeSort</th>\n",
              "      <th>RosterCount</th>\n",
              "      <th>Absent</th>\n",
              "      <th>Present</th>\n",
              "      <th>Released</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>01M034</td>\n",
              "      <td>5</td>\n",
              "      <td>Jan</td>\n",
              "      <td>08</td>\n",
              "      <td>8</td>\n",
              "      <td>62</td>\n",
              "      <td>131</td>\n",
              "      <td>1086</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>01M034</td>\n",
              "      <td>6</td>\n",
              "      <td>Feb</td>\n",
              "      <td>08</td>\n",
              "      <td>8</td>\n",
              "      <td>60</td>\n",
              "      <td>80</td>\n",
              "      <td>760</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>01M034</td>\n",
              "      <td>7</td>\n",
              "      <td>Mar</td>\n",
              "      <td>08</td>\n",
              "      <td>8</td>\n",
              "      <td>60</td>\n",
              "      <td>126</td>\n",
              "      <td>1175</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>01M034</td>\n",
              "      <td>8</td>\n",
              "      <td>Apr</td>\n",
              "      <td>08</td>\n",
              "      <td>8</td>\n",
              "      <td>60</td>\n",
              "      <td>80</td>\n",
              "      <td>697</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>01M034</td>\n",
              "      <td>9</td>\n",
              "      <td>May</td>\n",
              "      <td>08</td>\n",
              "      <td>8</td>\n",
              "      <td>60</td>\n",
              "      <td>95</td>\n",
              "      <td>1225</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     School  MonthCode CalMonth GradeLevel  GradeSort  RosterCount  Absent  \\\n",
              "217  01M034          5      Jan         08          8           62     131   \n",
              "227  01M034          6      Feb         08          8           60      80   \n",
              "237  01M034          7      Mar         08          8           60     126   \n",
              "247  01M034          8      Apr         08          8           60      80   \n",
              "257  01M034          9      May         08          8           60      95   \n",
              "\n",
              "     Present  Released  \n",
              "217     1086         0  \n",
              "227      760         0  \n",
              "237     1175         0  \n",
              "247      697         0  \n",
              "257     1225         0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "PwrAlJ9eKvMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "dc394998-184a-44ea-f73f-6ae20640d31c"
      },
      "cell_type": "code",
      "source": [
        "# Join all relevant dataframs\n",
        "schoolExplorer = school_data\n",
        "middleschoolDirectory = middleschoolDirectory.set_index('schooldbn')\n",
        "\n",
        "schoolExplorer = schoolExplorer.set_index('Location Code')\n",
        "nytdf = nytdf.set_index('DBN')\n",
        "\n",
        "explorer_nyt = schoolExplorer.join(nytdf)\n",
        "nytCombined = nytdf.join(schoolExplorer)\n",
        "\n",
        "nytCombined.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DataName</th>\n",
              "      <th>SchoolName1</th>\n",
              "      <th>SchoolName2</th>\n",
              "      <th>NumSHSATTestTakers</th>\n",
              "      <th>NumSpecializedOffers</th>\n",
              "      <th>OffersPerStudent</th>\n",
              "      <th>PctBlackOrHispanic</th>\n",
              "      <th>Adjusted Grade</th>\n",
              "      <th>New?</th>\n",
              "      <th>Other Location Code in LCGMS</th>\n",
              "      <th>...</th>\n",
              "      <th>Grade 8 Math - All Students Tested</th>\n",
              "      <th>Grade 8 Math 4s - All Students</th>\n",
              "      <th>Grade 8 Math 4s - American Indian or Alaska Native</th>\n",
              "      <th>Grade 8 Math 4s - Black or African American</th>\n",
              "      <th>Grade 8 Math 4s - Hispanic or Latino</th>\n",
              "      <th>Grade 8 Math 4s - Asian or Pacific Islander</th>\n",
              "      <th>Grade 8 Math 4s - White</th>\n",
              "      <th>Grade 8 Math 4s - Multiracial</th>\n",
              "      <th>Grade 8 Math 4s - Limited English Proficient</th>\n",
              "      <th>Grade 8 Math 4s - Economically Disadvantaged</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DBN</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20K187</th>\n",
              "      <td>THE CHRISTA MCAULIFFE SCHOOL\\I.S. 187</td>\n",
              "      <td>Intermediate School 187</td>\n",
              "      <td>The Christa McAuliffe School</td>\n",
              "      <td>251</td>\n",
              "      <td>205</td>\n",
              "      <td>75%</td>\n",
              "      <td>8%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>339.0</td>\n",
              "      <td>312.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>246.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>196.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21K239</th>\n",
              "      <td>MARK TWAIN I.S. 239 FOR THE GIFTED &amp; TALENTED</td>\n",
              "      <td>Intermediate School 239</td>\n",
              "      <td>The Mark Twain Intermediate School for the Gif...</td>\n",
              "      <td>336</td>\n",
              "      <td>196</td>\n",
              "      <td>46%</td>\n",
              "      <td>13%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>366.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03M054</th>\n",
              "      <td>J.H.S. 054 BOOKER T. WASHINGTON</td>\n",
              "      <td>Junior High School 54</td>\n",
              "      <td>The Booker T. Washington School</td>\n",
              "      <td>257</td>\n",
              "      <td>150</td>\n",
              "      <td>53%</td>\n",
              "      <td>23%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>34.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15K051</th>\n",
              "      <td>M.S. 51 WILLIAM ALEXANDER</td>\n",
              "      <td>Midde School 51</td>\n",
              "      <td>The William Alexander School</td>\n",
              "      <td>280</td>\n",
              "      <td>122</td>\n",
              "      <td>33%</td>\n",
              "      <td>28%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>160.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02M312</th>\n",
              "      <td>NEW YORK CITY LAB MIDDLE SCHOOL FOR COLLABORAT...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New York City Lab Middle School for Collaborat...</td>\n",
              "      <td>163</td>\n",
              "      <td>113</td>\n",
              "      <td>62%</td>\n",
              "      <td>8%</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>42.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 167 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 DataName  \\\n",
              "DBN                                                         \n",
              "20K187              THE CHRISTA MCAULIFFE SCHOOL\\I.S. 187   \n",
              "21K239      MARK TWAIN I.S. 239 FOR THE GIFTED & TALENTED   \n",
              "03M054                    J.H.S. 054 BOOKER T. WASHINGTON   \n",
              "15K051                          M.S. 51 WILLIAM ALEXANDER   \n",
              "02M312  NEW YORK CITY LAB MIDDLE SCHOOL FOR COLLABORAT...   \n",
              "\n",
              "                    SchoolName1  \\\n",
              "DBN                               \n",
              "20K187  Intermediate School 187   \n",
              "21K239  Intermediate School 239   \n",
              "03M054    Junior High School 54   \n",
              "15K051          Midde School 51   \n",
              "02M312                      NaN   \n",
              "\n",
              "                                              SchoolName2  NumSHSATTestTakers  \\\n",
              "DBN                                                                             \n",
              "20K187                       The Christa McAuliffe School                 251   \n",
              "21K239  The Mark Twain Intermediate School for the Gif...                 336   \n",
              "03M054                    The Booker T. Washington School                 257   \n",
              "15K051                       The William Alexander School                 280   \n",
              "02M312  New York City Lab Middle School for Collaborat...                 163   \n",
              "\n",
              "        NumSpecializedOffers OffersPerStudent PctBlackOrHispanic  \\\n",
              "DBN                                                                \n",
              "20K187                   205              75%                 8%   \n",
              "21K239                   196              46%                13%   \n",
              "03M054                   150              53%                23%   \n",
              "15K051                   122              33%                28%   \n",
              "02M312                   113              62%                 8%   \n",
              "\n",
              "       Adjusted Grade New? Other Location Code in LCGMS  \\\n",
              "DBN                                                       \n",
              "20K187            NaN  NaN                          NaN   \n",
              "21K239            NaN  NaN                          NaN   \n",
              "03M054            NaN  NaN                          NaN   \n",
              "15K051            NaN  NaN                          NaN   \n",
              "02M312            NaN  NaN                          NaN   \n",
              "\n",
              "                           ...                       \\\n",
              "DBN                        ...                        \n",
              "20K187                     ...                        \n",
              "21K239                     ...                        \n",
              "03M054                     ...                        \n",
              "15K051                     ...                        \n",
              "02M312                     ...                        \n",
              "\n",
              "       Grade 8 Math - All Students Tested  Grade 8 Math 4s - All Students  \\\n",
              "DBN                                                                         \n",
              "20K187                              339.0                           312.0   \n",
              "21K239                              366.0                           209.0   \n",
              "03M054                               34.0                             1.0   \n",
              "15K051                              160.0                            37.0   \n",
              "02M312                               42.0                             7.0   \n",
              "\n",
              "        Grade 8 Math 4s - American Indian or Alaska Native  \\\n",
              "DBN                                                          \n",
              "20K187                                                0.0    \n",
              "21K239                                                0.0    \n",
              "03M054                                                0.0    \n",
              "15K051                                                0.0    \n",
              "02M312                                                0.0    \n",
              "\n",
              "        Grade 8 Math 4s - Black or African American  \\\n",
              "DBN                                                   \n",
              "20K187                                          0.0   \n",
              "21K239                                          5.0   \n",
              "03M054                                          0.0   \n",
              "15K051                                          5.0   \n",
              "02M312                                          0.0   \n",
              "\n",
              "        Grade 8 Math 4s - Hispanic or Latino  \\\n",
              "DBN                                            \n",
              "20K187                                   0.0   \n",
              "21K239                                   4.0   \n",
              "03M054                                   0.0   \n",
              "15K051                                   5.0   \n",
              "02M312                                   0.0   \n",
              "\n",
              "       Grade 8 Math 4s - Asian or Pacific Islander Grade 8 Math 4s - White  \\\n",
              "DBN                                                                          \n",
              "20K187                                       246.0                    59.0   \n",
              "21K239                                        98.0                    99.0   \n",
              "03M054                                         0.0                     0.0   \n",
              "15K051                                         0.0                    22.0   \n",
              "02M312                                         3.0                     3.0   \n",
              "\n",
              "        Grade 8 Math 4s - Multiracial  \\\n",
              "DBN                                     \n",
              "20K187                            0.0   \n",
              "21K239                            3.0   \n",
              "03M054                            0.0   \n",
              "15K051                            0.0   \n",
              "02M312                            0.0   \n",
              "\n",
              "       Grade 8 Math 4s - Limited English Proficient  \\\n",
              "DBN                                                   \n",
              "20K187                                          0.0   \n",
              "21K239                                          0.0   \n",
              "03M054                                          0.0   \n",
              "15K051                                          0.0   \n",
              "02M312                                          0.0   \n",
              "\n",
              "       Grade 8 Math 4s - Economically Disadvantaged  \n",
              "DBN                                                  \n",
              "20K187                                        196.0  \n",
              "21K239                                         65.0  \n",
              "03M054                                          0.0  \n",
              "15K051                                          8.0  \n",
              "02M312                                          1.0  \n",
              "\n",
              "[5 rows x 167 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "xrBfvek3eRDX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "Picking, amending, and converting the necessary features into usable values for a predictive model."
      ]
    },
    {
      "metadata": {
        "id": "8qMLdsyIKvMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f3915de1-ce49-4b35-fd3f-6ec937e9cd23"
      },
      "cell_type": "code",
      "source": [
        "#Pick features\n",
        "features = ['Percent of Students Chronically Absent','Average Math Proficiency','Economic Need Index','Average ELA Proficiency', 'Latitude', 'Longitude', 'NumSHSATTestTakers', 'Student Attendance Rate']\n",
        "target = 'NumSHSATTestTakers'\n",
        "\n",
        "nytCombinedFiltered = nytCombined.query('NumSHSATTestTakers != 0')\n",
        "\n",
        "nytCombinedFiltered[features].describe()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Average Math Proficiency</th>\n",
              "      <th>Economic Need Index</th>\n",
              "      <th>Average ELA Proficiency</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>NumSHSATTestTakers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>530.000000</td>\n",
              "      <td>531.000000</td>\n",
              "      <td>530.000000</td>\n",
              "      <td>536.000000</td>\n",
              "      <td>536.000000</td>\n",
              "      <td>537.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.668849</td>\n",
              "      <td>0.657750</td>\n",
              "      <td>2.574906</td>\n",
              "      <td>40.738840</td>\n",
              "      <td>-73.915875</td>\n",
              "      <td>47.204842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.486156</td>\n",
              "      <td>0.191868</td>\n",
              "      <td>0.375443</td>\n",
              "      <td>0.086535</td>\n",
              "      <td>0.075129</td>\n",
              "      <td>61.158551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.890000</td>\n",
              "      <td>0.059000</td>\n",
              "      <td>1.910000</td>\n",
              "      <td>40.507803</td>\n",
              "      <td>-74.243221</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.270000</td>\n",
              "      <td>0.545500</td>\n",
              "      <td>2.280000</td>\n",
              "      <td>40.671637</td>\n",
              "      <td>-73.955888</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.610000</td>\n",
              "      <td>0.710000</td>\n",
              "      <td>2.490000</td>\n",
              "      <td>40.728875</td>\n",
              "      <td>-73.919367</td>\n",
              "      <td>26.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.980000</td>\n",
              "      <td>0.806500</td>\n",
              "      <td>2.790000</td>\n",
              "      <td>40.820391</td>\n",
              "      <td>-73.880919</td>\n",
              "      <td>46.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.190000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>3.930000</td>\n",
              "      <td>40.899321</td>\n",
              "      <td>-73.713022</td>\n",
              "      <td>394.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Average Math Proficiency  Economic Need Index  Average ELA Proficiency  \\\n",
              "count                530.000000           531.000000               530.000000   \n",
              "mean                   2.668849             0.657750                 2.574906   \n",
              "std                    0.486156             0.191868                 0.375443   \n",
              "min                    1.890000             0.059000                 1.910000   \n",
              "25%                    2.270000             0.545500                 2.280000   \n",
              "50%                    2.610000             0.710000                 2.490000   \n",
              "75%                    2.980000             0.806500                 2.790000   \n",
              "max                    4.190000             0.938000                 3.930000   \n",
              "\n",
              "         Latitude   Longitude  NumSHSATTestTakers  \n",
              "count  536.000000  536.000000          537.000000  \n",
              "mean    40.738840  -73.915875           47.204842  \n",
              "std      0.086535    0.075129           61.158551  \n",
              "min     40.507803  -74.243221            6.000000  \n",
              "25%     40.671637  -73.955888           15.000000  \n",
              "50%     40.728875  -73.919367           26.000000  \n",
              "75%     40.820391  -73.880919           46.000000  \n",
              "max     40.899321  -73.713022          394.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "KEM3vGiUchZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "Set of functions to normalize data and remove percentages."
      ]
    },
    {
      "metadata": {
        "id": "wOz_0BSYcf36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert string valued percentages into floats within [0,1]\n",
        "def removePercent(df,columnNames):\n",
        "  for i in range(len(columnNames)):\n",
        "    col_string = df[columnNames[i]].str\n",
        "    df[columnNames[i]] = col_string.rstrip('%').astype('float') / 100.0\n",
        "    pass\n",
        "  return df\n",
        "  \n",
        "# for values in arbitrary ranges, normalize across [-1,1]\n",
        "def normalize_series(df, labels):\n",
        "  for label in labels:\n",
        "#     for sigma = 1 standard deviation, ~97% of datapoints should be within [-3sigma,sigma]\n",
        "    df[label] = (df[label] - df[label].mean()) / (3 * df[label].std())\n",
        "    pass\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWuzcicOfX_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "4372fe96-1809-4425-f24c-2cc69feeed40"
      },
      "cell_type": "code",
      "source": [
        "# Applying helpers to dataset\n",
        "\n",
        "nytApplyFilters = removePercent(nytCombinedFiltered.copy(), ['Student Attendance Rate', 'Percent of Students Chronically Absent'])\n",
        "\n",
        "nytApplyFilters = normalize_series(nytApplyFilters, ['Average Math Proficiency', 'Average ELA Proficiency', 'Latitude', 'Longitude'])\n",
        "\n",
        "nytApplyFilters = nytApplyFilters.dropna(subset=[target])\n",
        "nytApplyFilters.fillna(0,inplace = True)\n",
        "\n",
        "nytApplyFilters[features].describe()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Percent of Students Chronically Absent</th>\n",
              "      <th>Average Math Proficiency</th>\n",
              "      <th>Economic Need Index</th>\n",
              "      <th>Average ELA Proficiency</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>NumSHSATTestTakers</th>\n",
              "      <th>Student Attendance Rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>537.000000</td>\n",
              "      <td>5.370000e+02</td>\n",
              "      <td>537.000000</td>\n",
              "      <td>5.370000e+02</td>\n",
              "      <td>5.370000e+02</td>\n",
              "      <td>5.370000e+02</td>\n",
              "      <td>537.000000</td>\n",
              "      <td>537.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.196108</td>\n",
              "      <td>7.939025e-17</td>\n",
              "      <td>0.650400</td>\n",
              "      <td>-2.514025e-16</td>\n",
              "      <td>1.860378e-14</td>\n",
              "      <td>-5.659532e-14</td>\n",
              "      <td>47.204842</td>\n",
              "      <td>0.913762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.150459</td>\n",
              "      <td>3.311496e-01</td>\n",
              "      <td>0.202953</td>\n",
              "      <td>3.311496e-01</td>\n",
              "      <td>3.330222e-01</td>\n",
              "      <td>3.330222e-01</td>\n",
              "      <td>61.158551</td>\n",
              "      <td>0.151724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.340184e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.903301e-01</td>\n",
              "      <td>-8.899587e-01</td>\n",
              "      <td>-1.452364e+00</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>-2.666146e-01</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>-2.618291e-01</td>\n",
              "      <td>-2.579054e-01</td>\n",
              "      <td>-1.773479e-01</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.920000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.170000</td>\n",
              "      <td>-3.349339e-02</td>\n",
              "      <td>0.708000</td>\n",
              "      <td>-7.538267e-02</td>\n",
              "      <td>-3.731623e-02</td>\n",
              "      <td>-1.418521e-02</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.270000</td>\n",
              "      <td>2.133409e-01</td>\n",
              "      <td>0.806000</td>\n",
              "      <td>1.820910e-01</td>\n",
              "      <td>3.133669e-01</td>\n",
              "      <td>1.541464e-01</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.042978e+00</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>1.203107e+00</td>\n",
              "      <td>6.181726e-01</td>\n",
              "      <td>9.000141e-01</td>\n",
              "      <td>394.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Percent of Students Chronically Absent  Average Math Proficiency  \\\n",
              "count                              537.000000              5.370000e+02   \n",
              "mean                                 0.196108              7.939025e-17   \n",
              "std                                  0.150459              3.311496e-01   \n",
              "min                                  0.000000             -5.340184e-01   \n",
              "25%                                  0.100000             -2.666146e-01   \n",
              "50%                                  0.170000             -3.349339e-02   \n",
              "75%                                  0.270000              2.133409e-01   \n",
              "max                                  1.000000              1.042978e+00   \n",
              "\n",
              "       Economic Need Index  Average ELA Proficiency      Latitude  \\\n",
              "count           537.000000             5.370000e+02  5.370000e+02   \n",
              "mean              0.650400            -2.514025e-16  1.860378e-14   \n",
              "std               0.202953             3.311496e-01  3.330222e-01   \n",
              "min               0.000000            -5.903301e-01 -8.899587e-01   \n",
              "25%               0.538000            -2.618291e-01 -2.579054e-01   \n",
              "50%               0.708000            -7.538267e-02 -3.731623e-02   \n",
              "75%               0.806000             1.820910e-01  3.133669e-01   \n",
              "max               0.938000             1.203107e+00  6.181726e-01   \n",
              "\n",
              "          Longitude  NumSHSATTestTakers  Student Attendance Rate  \n",
              "count  5.370000e+02          537.000000               537.000000  \n",
              "mean  -5.659532e-14           47.204842                 0.913762  \n",
              "std    3.330222e-01           61.158551                 0.151724  \n",
              "min   -1.452364e+00            6.000000                 0.000000  \n",
              "25%   -1.773479e-01           15.000000                 0.920000  \n",
              "50%   -1.418521e-02           26.000000                 0.940000  \n",
              "75%    1.541464e-01           46.000000                 0.960000  \n",
              "max    9.000141e-01          394.000000                 1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "MaEejyNxoPyZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setting up features as a vector\n",
        "# Separating training and testing for k-fold vallidation\n",
        "num_folds = 10\n",
        "N = nytApplyFilters.shape[0]\n",
        "\n",
        "test_set = nytApplyFilters[0: N / num_folds]\n",
        "train_set = nytApplyFilters[N / num_folds : N]\n",
        "\n",
        "test_set = test_set.filter(features)\n",
        "train_set = train_set.filter(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mkygk68pq3Q7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c24d358d-9997-4b3f-c3a8-d8b62259552c"
      },
      "cell_type": "code",
      "source": [
        "# X and Y from train sets\n",
        "\n",
        "X = train_set.drop(target, axis=1).values\n",
        "\n",
        "# Using the output as a percentage\n",
        "\n",
        "Y = train_set[[target]].values\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(484, 7)\n",
            "(484, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BrNily2drusm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Neural Network\n"
      ]
    },
    {
      "metadata": {
        "id": "4QFsFK9wrt0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "96c31223-fd64-4046-f5bf-5b1b72d670e2"
      },
      "cell_type": "code",
      "source": [
        "numFeatures = len(features)\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "activationFuntion = 'relu'\n",
        "model.add(Dense(20, input_dim=numFeatures-1, activation='relu'))\n",
        "# model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "# model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "# model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae','accuracy'])\n",
        "\n",
        "\n",
        "#Train\n",
        "model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=200,\n",
        "    shuffle=True,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "\n",
        "X_test = test_set.drop(target, axis=1).values\n",
        "Y_test = test_set[[target]].values\n",
        "\n",
        "test_error_rate = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "testErrorArray.append(test_error_rate)\n",
        "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))\n",
        "print(\"Here: \\n%s: %.8f%%\" % (model.metrics_names[2], test_error_rate[2]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean squared error (MSE) for the test data set is: [28634.947302476416, 142.72677036501327, 0.0]\n",
            "Here: \n",
            "acc: 0.00000000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W99G64DAUbSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Best \"acc\" achieved by this prediction network: 0.01887%.\n",
        "Best MSE on real valued Num of SHSAT Test Takers: 142.65"
      ]
    },
    {
      "metadata": {
        "id": "odaWq-BYUmLn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Radial Basis Function (RBF) Model\n",
        "\n",
        "I'm going to try a Radial Basis Function Model to minimize in-sample Mean Squared Error.\n",
        "\n",
        "Value to beat: 142.65."
      ]
    },
    {
      "metadata": {
        "id": "YsoiTEKBVRst",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Intuition\n",
        "\n",
        "You may not feel the connection immediately, even by looking at the math, but I think of radial basis functions as probability distributions around certain points.\n",
        "\n",
        "This is the function I've chosen; I've chosen it because the brilliant Dr. Abu Mustafa of the online Caltech ML course on YouTube picked it out when explaining the math behind this. My whole model is based around it:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Large\n",
        "f(\\mathbf{x}) = \\sum_{k=1}^K w_ke^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_k} \\| ^ 2}\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "Utq8_5O4dSNQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "What does this mean? Well, basically, it follows directly from the idea of continuity. What if our target function predicting $y$ was continuous? If that were true, then for a point at a certain location, we could assume that points close to that location would be kind of close to the value at that location. It's becomes a cumulative probability distribution, where every point in the training set is understood to have an area of influence that tapers off based on how far away an input is from that point. It also has a weight, or the value that area of influence imposes on the input.\n",
        "\n",
        "So the influence on any input $ x $ is dependent on the exponential distribution to a given point $ \\mu $ by the following equation:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\forall \\mu, f(\\mathbf{x}) \\propto e^{-\\lambda \\mathbf{ \\| x - \\mu \\| } ^ 2}\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CGRN6SR6dWM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now in this case $ \\lambda$ is the rate at which the influence of any given point tapers off as the input moves farther away for a $\\mu$. Changing the value of lambda decides how far close to each $\\mu$ an input has to be to get the influence.\n",
        "and by that logic, we can conclude that if we have $K$ $\\mu$'s,\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "f(\\mathbf{x}) = \\sum_{k=1}^K w_ke^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_k} \\| ^ 2}\n",
        "\\end{equation}\n",
        "\n",
        "Notice how I've been really deliberate in calling each point $\\mu$. If you know some statistics you picked up on the fact that its a symbol that denotes a mean. The reason for this is simply to improve on computation. Some datasets have records on the order of $10^5$. That's ridiculous, and it would make no sense to to run the whole calculation. So instead we generate a set of $K$ different $\\mu$s, or 'K-means'. These means are representative points for the entire dataset given to us. I haven't come across any specific rule of thumb while choosing $K$, so what I'm going to do is have $K \\propto \\log{N}$. After all, the whole point of having $K$ is to reduce the number of computations you have to go through."
      ]
    },
    {
      "metadata": {
        "id": "k8ev1VnH7v0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Process\n"
      ]
    },
    {
      "metadata": {
        "id": "JERJGOtzhRl6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 1:\n",
        "K-means clustering on feature sets.\n",
        "\n",
        "### Step 2:\n",
        "Define the RBF equation as follows.\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "y = \\sum_{k=1}^K w_ke^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_k} \\| ^ 2}\n",
        "\\end{equation}\n",
        "\n",
        "with precision rate $\\lambda$ where $\\mu_k$ is a mean point for all $K$- means.\n",
        "\n",
        "### Step 3:\n",
        "Build model to find least square values for all weights $w_k$.\n",
        "\n",
        "### Step 4:\n",
        "Test model with different precision rates, try to minimize cost."
      ]
    },
    {
      "metadata": {
        "id": "ueUm9Y8vcQhl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: K means clustering"
      ]
    },
    {
      "metadata": {
        "id": "01zZZBP_MCDl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lloyd's algorithm\n",
        "I'm going to implement K-means clustering using Lloyd's algorithm on the feature vector $ \\mathbf{x} $,\n",
        "using the following iterative implementation for Lloyd's algotihm.\n",
        "\n",
        "\\begin{equation} \\mathbf{\\mu_0} = random \\in {\\mathbf{x}} \\end{equation}\n",
        "\n",
        "\\begin{equation} \\mathbf{S_0} =  \\{ \\mathbf{x} : \\forall \\mu, \\| \\mathbf{x} - \\mathbf{\\mu_0} \\| \\leq \\| \\mathbf{x} - \\mathbf{\\mu} \\|\\} \\end{equation}\n",
        "\n",
        "\\begin{equation} \\mathbf{\\mu_{n+1}} = \\frac{1}{\\mathbf{\\|S_n\\|}} \\sum_{\\mathbf{x} \\in \\mathbf{S_n}} \\mathbf{x} \\end{equation}\n",
        "\n",
        "\\begin{equation} \\mathbf{S_{n+1}} =  \\{ \\mathbf{x} : \\forall \\mu, \\| \\mathbf{x} - \\mathbf{\\mu_{n+1}} \\| \\leq \\| \\mathbf{x} - \\mathbf{\\mu} \\|\\} \\end{equation}\n"
      ]
    },
    {
      "metadata": {
        "id": "qMIPF-qxcOZ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  Lloyd's algorithm: Implmentation.\n",
        "# Assume feature engineering has been done.\n",
        "class LloydMeans:\n",
        "  \n",
        "  def __init__ (self, k, df, labels):\n",
        "    self.pointMatrix = df[labels].as_matrix()\n",
        "    self.df = df\n",
        "    self.labels = labels\n",
        "    self.k = k\n",
        "    pass\n",
        "  \n",
        "  def assign_means(self, num_trials, num_iterations):\n",
        "    best_error = float(\"inf\") # Best error is positive infinity\n",
        "    best_mu = []\n",
        "    best_meanSet = []\n",
        "    clustered_features = self.labels\n",
        "    pointMatrix = self.pointMatrix\n",
        "    k = self.k\n",
        "    df = self.df\n",
        "\n",
        "    for trial in range(num_trials):\n",
        "      mu = self.init_mu(df, clustered_features, k)\n",
        "\n",
        "      for i in range(num_iterations):\n",
        "        #iteratively update the clusters and cluster means nunm_iterations times\n",
        "        meanSet = self.update_clusters(pointMatrix, mu, k)\n",
        "        mu = self.update_cluster_points(meanSet, mu, k)\n",
        "        pass\n",
        "\n",
        "      #calculate error of current mu model\n",
        "      model_error = self.calculate_total_error(meanSet, mu)\n",
        "\n",
        "      print(\"MSE for trial {} : {}\".format(trial, model_error))\n",
        "\n",
        "      #always take the best model w/ minimum error\n",
        "      if(model_error < best_error):\n",
        "        best_error = model_error\n",
        "        best_mu = mu\n",
        "        best_meanSet = meanSet\n",
        "        pass\n",
        "      pass\n",
        "    print(\"Best error is {}\".format(best_error))\n",
        "#     df = assign_final_clusters(pointMatrix, best_mu, k, self.df)\n",
        "    return best_mu\n",
        "  \n",
        "  def normalize_matrix(self, df, labels):\n",
        "    mat = df[labels].as_matrix()\n",
        "    # transform all datapoints so ~97% of values are in [-1, 1]\n",
        "    for i in range(mat.shape[0]):\n",
        "      for j in range(mat.shape[1]):\n",
        "        mat[i][j] = (mat[i][j] - df[labels[j]].mean()) / (3 * df[labels[j]].std())\n",
        "        pass\n",
        "      pass\n",
        "    return mat\n",
        "\n",
        "  def init_mu(self, df, labels, k):\n",
        "      mu = np.zeros((k, len(labels)))\n",
        "      for i in range(len(labels)):\n",
        "        for j in range(k):\n",
        "#           initialize random float in [-1, 1]\n",
        "          offset = np.random.ranf() * 6 - 3\n",
        "          mu[j][i] = df[labels[i]].mean() + offset\n",
        "      return mu\n",
        "\n",
        "  def update_clusters(self, pointMatrix, mu, k):\n",
        "    meanSet = [[] for i in range(k)]\n",
        "  #   iterate over points\n",
        "    for i in range(pointMatrix.shape[0]):\n",
        "      minIndex = 0\n",
        "      minDistance = np.linalg.norm(pointMatrix[i] - mu[minIndex])\n",
        "  #     iterate over mu (mean points)\n",
        "      for j in range(k):\n",
        "        dist = np.linalg.norm(pointMatrix[i] - mu[j])\n",
        "  #       pick j with the minimum distance from i\n",
        "        if(dist < minDistance):\n",
        "          minDistance = dist\n",
        "          minIndex = j\n",
        "          pass\n",
        "        pass\n",
        "    #   Add point i to mu[j]'s cluster'\n",
        "      meanSet[minIndex].append(pointMatrix[i])\n",
        "      pass\n",
        "    return meanSet\n",
        "\n",
        "  def assign_final_clusters(self, pointMatrix, mu, k, df):\n",
        "  #   iterate over points (repeating update_clusters)\n",
        "    for i in range(pointMatrix.shape[0]):\n",
        "      minIndex = 0\n",
        "      minDistance = np.linalg.norm(pointMatrix[i] - mu[minIndex])\n",
        "  #     iterate over mu (mean points)\n",
        "      for j in range(k):\n",
        "        dist = np.linalg.norm(pointMatrix[i] - mu[j])\n",
        "  #       pick j with the minimum distance from i\n",
        "        if(dist < minDistance):\n",
        "          minDistance = dist\n",
        "          minIndex = j\n",
        "          pass\n",
        "        pass\n",
        "    #   Assign final cluster values to dataframe\n",
        "      df.loc[df.index[i], 'cluster'] = minIndex\n",
        "      pass\n",
        "    return df\n",
        "\n",
        "  def update_cluster_points(self, meanSet, mu, k):\n",
        "  #   iterate over mu\n",
        "    for i in range(k):\n",
        "      set_sum = np.zeros(mu[i].shape)\n",
        "  #     iterate over mu[i]'s cluster'\n",
        "      for j in range(len(meanSet[i])):\n",
        "        # sum up all the positions of each point\n",
        "        set_sum += meanSet[i][j]\n",
        "        pass\n",
        "      # update mu to the average of each point in mu's cluster\n",
        "      if len(meanSet[i]) != 0:\n",
        "        mu[i] = set_sum / len(meanSet[i])\n",
        "        pass\n",
        "      pass\n",
        "    return mu\n",
        "\n",
        "  def calculate_total_error(self, meanSet, mu):\n",
        "    mserror = 0\n",
        "    N = 0\n",
        "    for i in range(self.k):\n",
        "      for j in range(len(meanSet[i])):\n",
        "        error = np.linalg.norm(meanSet[i][j] - mu[i])\n",
        "        mserror += error * error\n",
        "        N += 1\n",
        "        pass\n",
        "      pass\n",
        "    return mserror / N\n",
        "  \n",
        "  pass\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyosdWSgSk_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3fc40140-7b5a-46ce-aaaf-06fac7743dc5"
      },
      "cell_type": "code",
      "source": [
        "# Lloyd's Algorithm- application\n",
        "\n",
        "# Set up X as independent dataframe with features vector\n",
        "clusterDF = train_set\n",
        "featureLabels = copy.deepcopy(features)\n",
        "featureLabels.remove(target)\n",
        "\n",
        "# Let K increase logarithmically with N.\n",
        "# This will help optimize clustering and create good representatives.\n",
        "\n",
        "train_N = clusterDF.shape[0]\n",
        "K = 5 * int(math.floor(math.log(train_N)))\n",
        "\n",
        "lloyd = LloydMeans(K, clusterDF, featureLabels)\n",
        "mu = lloyd.assign_means(num_iterations = 30, num_trials = 10)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE for trial 0 : 0.129738119818\n",
            "MSE for trial 1 : 0.247218580904\n",
            "MSE for trial 2 : 0.278175703223\n",
            "MSE for trial 3 : 0.298511482234\n",
            "MSE for trial 4 : 0.195443270384\n",
            "MSE for trial 5 : 0.178731272948\n",
            "MSE for trial 6 : 0.175493791762\n",
            "MSE for trial 7 : 0.174968488729\n",
            "MSE for trial 8 : 0.266410064383\n",
            "MSE for trial 9 : 0.167184996173\n",
            "Best error is 0.129738119818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qLe05DkAn8gj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining the Radial Basis Function"
      ]
    },
    {
      "metadata": {
        "id": "2pCx-7iyL7or",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is the function again, just to remind myself, and because LaTEX is awesome.\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "f(\\mathbf{x}) = \\sum_{k=1}^K w_ke^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_k} \\| ^ 2}\n",
        "\\end{equation}\n",
        "\n",
        "**Note: $\\lambda$ in the code is \"precision_rate\". This is because the phrase \"lambda\" reads as the start of a lambda function, so I can't use it to name a variable.**"
      ]
    },
    {
      "metadata": {
        "id": "iwKcfGqHnatV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _predict (mu, w, x, precision_rate):\n",
        "#   Sum over mu\n",
        "  sum = 0\n",
        "  for i in range(mu.shape[0]):\n",
        "#     distance between point x and mu\n",
        "    dist = np.linalg.norm(x - mu[i])\n",
        "#   square distance, multiply by lambda\n",
        "    exponent = (- precision_rate) * dist * dist\n",
        "#   multiply exponent by weight, add to total\n",
        "    sum += (w[i] * math.exp(exponent))\n",
        "    pass\n",
        "  return sum\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1C_y7Kbv-wt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a good start, but we have to visualize this as a matrix instead of a function before we can solve for $ \\mathbf{w} $\n",
        "\n",
        "We're going to do this by creating the following matrix:\n",
        "\n",
        "\\begin{equation} \\mathbf{A_{1,K}} = \n",
        "\\begin{pmatrix} \n",
        "e^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_1} \\| ^ 2} & \\cdots & e^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_K} \\| ^ 2}\n",
        "\\end{pmatrix} \\end{equation}\n",
        "\n",
        "This way, the new equation is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix} \n",
        "e^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_1} \\| ^ 2} & \\cdots & e^{-\\lambda \\| \\mathbf{x} - \\mathbf{\\mu_K} \\| ^ 2}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "w_1 \\\\ \\vdots \\\\ w_K\n",
        "\\end{bmatrix} = y\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "or\n",
        "\n",
        "\\begin{equation} \\mathbf{Aw} = y \\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "DvOVtGX2ogU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict (mu, w, x, precision_rate):\n",
        "  A = np.zeros((1, mu.shape[0]))\n",
        "  for i in range(mu.shape[0]):\n",
        "#   distance between point x and mu\n",
        "    dist = np.linalg.norm(x - mu[i])\n",
        "#   square distance, multiply by lambda\n",
        "    exponent = (- precision_rate) * dist * dist\n",
        "    A[0][i] = math.exp(exponent)\n",
        "    pass\n",
        "  return np.dot(A, w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WiiU8GRvjmpW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Training.\n"
      ]
    },
    {
      "metadata": {
        "id": "6YnFaazULwcq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is going to be the most fun part.\n",
        "Extend our previous matrix $ \\mathbf{A} $ to all N points in our dataset, creating a K by N matrix.\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{A_{N,K}} = \n",
        "\\begin{bmatrix} \n",
        "e^{-\\lambda \\| \\mathbf{x_1} - \\mathbf{\\mu_1} \\| ^ 2} & \n",
        "e^{-\\lambda \\| \\mathbf{x_1} - \\mathbf{\\mu_2} \\| ^ 2} & \\cdots & \n",
        "e^{-\\lambda \\| \\mathbf{x_1} - \\mathbf{\\mu_K} \\| ^ 2} \n",
        "\\\\\n",
        "e^{-\\lambda \\| \\mathbf{x_2} - \\mathbf{\\mu_1} \\| ^ 2} & \n",
        "e^{-\\lambda \\| \\mathbf{x_2} - \\mathbf{\\mu_2} \\| ^ 2} & \\cdots &\n",
        "e^{-\\lambda \\| \\mathbf{x_2} - \\mathbf{\\mu_K} \\| ^ 2}\n",
        "\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\n",
        "\\\\\n",
        "e^{-\\lambda \\| \\mathbf{x_N} - \\mathbf{\\mu_1} \\| ^ 2} &\n",
        "e^{-\\lambda \\| \\mathbf{x_N} - \\mathbf{\\mu_2} \\| ^ 2} &\\cdots &\n",
        "e^{-\\lambda \\| \\mathbf{x_N} - \\mathbf{\\mu_K} \\| ^ 2} \n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Now,\n",
        "\n",
        "\\begin{equation} \\begin{bmatrix} \n",
        "e^{-\\lambda \\| \\mathbf{x_1} - \\mathbf{\\mu_1} \\| ^ 2} & \n",
        "e^{-\\lambda \\| \\mathbf{x_1} - \\mathbf{\\mu_2} \\| ^ 2} & \\cdots & \n",
        "e^{-\\lambda \\| \\mathbf{x_1} - \\mathbf{\\mu_K} \\| ^ 2} \n",
        "\\\\\n",
        "e^{-\\lambda \\| \\mathbf{x_2} - \\mathbf{\\mu_1} \\| ^ 2} & \n",
        "e^{-\\lambda \\| \\mathbf{x_2} - \\mathbf{\\mu_2} \\| ^ 2} & \\cdots &\n",
        "e^{-\\lambda \\| \\mathbf{x_2} - \\mathbf{\\mu_K} \\| ^ 2}\n",
        "\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\n",
        "\\\\\n",
        "e^{-\\lambda \\| \\mathbf{x_N} - \\mathbf{\\mu_1} \\| ^ 2} &\n",
        "e^{-\\lambda \\| \\mathbf{x_N} - \\mathbf{\\mu_2} \\| ^ 2} &\\cdots &\n",
        "e^{-\\lambda \\| \\mathbf{x_N} - \\mathbf{\\mu_K} \\| ^ 2} \n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_K\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\n",
        "\\end{bmatrix} \\end{equation}\n",
        "\n",
        "or,\n",
        "\n",
        "\\begin{equation} \\mathbf{Aw = y} \\end{equation}\n",
        "\n",
        "So, to solve for $ \\mathbf{w} $ given a reasonable lambda we use the old Math 415 least squares regression equation\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{w = (A^TA)^{-1}A^Ty}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We're also going to write a loss function to look for a good MSE"
      ]
    },
    {
      "metadata": {
        "id": "5s3asd572CnR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train (X, Y, precision_rate, mu):\n",
        "  A = np.zeros((X.shape[0], mu.shape[0]))\n",
        "#   iterate over X\n",
        "  for i in range(X.shape[0]):\n",
        "#     iterate over mu\n",
        "    for j in range(mu.shape[0]):\n",
        "#     create vectors for x and mu\n",
        "      _x = np.transpose([X[i]])\n",
        "      _mu = np.transpose([mu[j]])\n",
        "#     Take the distance between point x and mu\n",
        "      dist = np.linalg.norm(_x - _mu)\n",
        "  #   square distance, multiply by lambda\n",
        "      exponent = (- precision_rate) * dist * dist\n",
        "      A[i][j] = math.exp(exponent)\n",
        "      pass\n",
        "    pass\n",
        "  \n",
        "#   Get ATA\n",
        "  transpose = np.transpose(A)\n",
        "  ATA = np.dot(transpose, A)\n",
        "  \n",
        "#   Invert ATA\n",
        "  pseudoInv = np.linalg.inv(ATA)\n",
        "  \n",
        "#   Take(ATA)^-1 ATy\n",
        "  res = np.dot(transpose, Y)\n",
        "  res = np.dot(pseudoInv, res)\n",
        "  return res\n",
        "  \n",
        "  \n",
        "def get_loss(X, Y, precision_rate, mu, w):\n",
        "  mse = 0\n",
        "  count = 0\n",
        "  for i in range(X.shape[0]):\n",
        "    predicted = predict(mu, w, X[i], precision_rate)\n",
        "    err = Y[i] - predicted\n",
        "    mse += err * err\n",
        "    count += 1\n",
        "    pass\n",
        "  res = mse / count\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXYFITEc4rVX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Application"
      ]
    },
    {
      "metadata": {
        "id": "Le6sTNSZD0Ow",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I've now written more latex than code and tested none of the code. Let's see if anything happens.\n",
        "I'm going to put everything in a for loop that tries different lambdas."
      ]
    },
    {
      "metadata": {
        "id": "w5rKOr334pm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ec5fffa6-fdbc-45eb-f0c2-a7183c6c98d1"
      },
      "cell_type": "code",
      "source": [
        "step = 0.000005\n",
        "precision_rate = 0.0005\n",
        "X = train_set.drop(target, axis=1).values\n",
        "Y = train_set[[target]].values\n",
        "\n",
        "test_X = test_set.drop(target, axis=1).values\n",
        "test_Y = test_set[[target]].values\n",
        "\n",
        "best_precision = precision_rate\n",
        "best_error = float('inf')\n",
        "\n",
        "for i in range(20):\n",
        "#   Train for given precision rate\n",
        "  w = train(X, Y, precision_rate, mu)\n",
        "#   Get the error\n",
        "  current_error = get_loss(test_X, test_Y, precision_rate, mu, w)\n",
        "  print('MSE for lambda {} was {}'.format(precision_rate, current_error))\n",
        "  \n",
        "#   Replace the best error and rate if a better set is found\n",
        "  if current_error < best_error:\n",
        "    best_error = current_error\n",
        "    best_precision = precision_rate\n",
        "    pass\n",
        "  precision_rate += step\n",
        "  pass\n",
        "\n",
        "print('Best MSE was {} with precision rate {}'.format(best_error, best_precision))\n",
        "  "
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE for lambda 0.0005 was [[21813.18868148]]\n",
            "MSE for lambda 0.000505 was [[29815.91504766]]\n",
            "MSE for lambda 0.00051 was [[33550.49807164]]\n",
            "MSE for lambda 0.000515 was [[68577.65349522]]\n",
            "MSE for lambda 0.00052 was [[30218.1315357]]\n",
            "MSE for lambda 0.000525 was [[37512.12033987]]\n",
            "MSE for lambda 0.00053 was [[14239.39156975]]\n",
            "MSE for lambda 0.000535 was [[34363.78831057]]\n",
            "MSE for lambda 0.00054 was [[28831.84147649]]\n",
            "MSE for lambda 0.000545 was [[24670.82533999]]\n",
            "MSE for lambda 0.00055 was [[20519.23974181]]\n",
            "MSE for lambda 0.000555 was [[11413.12614696]]\n",
            "MSE for lambda 0.00056 was [[65123.83192398]]\n",
            "MSE for lambda 0.000565 was [[27451.04504692]]\n",
            "MSE for lambda 0.00057 was [[30581.46086842]]\n",
            "MSE for lambda 0.000575 was [[143328.08564603]]\n",
            "MSE for lambda 0.00058 was [[25851.86104825]]\n",
            "MSE for lambda 0.000585 was [[299360.53023674]]\n",
            "MSE for lambda 0.00059 was [[27242.45876025]]\n",
            "MSE for lambda 0.000595 was [[27628.24873]]\n",
            "Best MSE was [[11413.12614696]] with precision rate 0.000555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0q8gGiVkIZ4c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Review:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lCFAVnEihrnT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### ...Fuck.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "MSE for lambda 0.00055 was [[133638.7555843]]\n",
        "MSE for lambda 0.00054 was [[28831.84147649]]\n",
        "MSE for lambda 0.000545 was [[24670.82533999]]\n",
        "MSE for lambda 0.00055 was [[20519.23974181]]\n",
        "MSE for lambda 0.000555 was [[11413.12614696]]\n",
        "MSE for lambda 0.00056 was [[65123.83192398]]\n",
        "MSE for lambda 0.000565 was [[27451.04504692]]\n",
        "MSE for lambda 0.00057 was [[30581.46086842]]\n",
        "MSE for lambda 0.000575 was [[143328.08564603]]\n",
        "MSE for lambda 0.00058 was [[25851.86104825]]\n",
        "MSE for lambda 0.000585 was [[299360.53023674]]\n",
        "MSE for lambda 0.00059 was [[27242.45876025]]\n",
        "MSE for lambda 0.000595 was [[27628.24873]]\n",
        "Best MSE was [[11413.12614696]] with precision rate 0.000555\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "iuoGZY80Lnwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Possible issues:\n",
        " - N too small.\n",
        " - K too small.\n",
        " - Features are too few.\n",
        " - lambda uniform across means.\n",
        " - model not complex enough.\n",
        " - Some stupid mistake somewhere scales values wrong. You know, by a factor of $10^5$.\n",
        " \n",
        "#### Possible Solutions:\n",
        "  - Fuck this dataset. This dataset is horrible. 400 points, 7 features.\n",
        "  - Try to find/ implement variable lambda, and use iterative algorithm to regress w and lambda\n",
        "  - Implement RBF Network with multiple layers. This would be ridiculous in terms of debugging and time commitment."
      ]
    },
    {
      "metadata": {
        "id": "w7Aj2591U-b4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Refining the RBF: Multiple $ \\lambda$s"
      ]
    },
    {
      "metadata": {
        "id": "vDda4ORvD_m2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The thing about the radial basis function is that it works on distributions with two values, not one. Some points should have stronger influence areas than others. But our model applies a single $ \\lambda$ to all means.\n",
        "\n",
        "Now I'm going to try to beat the ridiculous MSE from the single $\\lambda$ model of 15223.95151741 by using multiple $\\lambda$s, so the new function will be as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "f(\\mathbf{x}) = \\sum_{k=1}^K w_ke^{-\\lambda_k \\| \\mathbf{x} - \\mathbf{\\mu_k} \\| ^ 2}\n",
        "\\end{equation}\n",
        "\n",
        "Here's the thing: this is a LOT more complicated than it looks.\n",
        "When we were optimizing for the linear weight vector $\\mathbf{w}$, we were able to pull off a basic linear algebra trick, but $\\lambda$ affects the equiation nonlinearly. That means the only way to train this model for $\\lambda$ is through stochastic gradient descent, which can be done, but it is in no way ideal in this situation and is going to depend entirely on how accurate my math is."
      ]
    },
    {
      "metadata": {
        "id": "ReqvOwyiEGCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1: Redefining prediction function"
      ]
    },
    {
      "metadata": {
        "id": "taEAGHCqETeN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm going to call this new function forward, because the process by which I train will also act on a single x, and I'm going to call it backward. The reason is that this model is going to update itself every time it gets a new variable."
      ]
    },
    {
      "metadata": {
        "id": "Q4FjFKqLEk9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward (mu, w, x, precision_rate):\n",
        "  A = np.zeros((1, mu.shape[0]))\n",
        "  for i in range(mu.shape[0]):\n",
        "#   distance between point x and mu\n",
        "    dist = np.linalg.norm(x - mu[i])\n",
        "#   square distance, multiply by lambda\n",
        "    exponent = (- precision_rate[i]) * dist * dist\n",
        "    A[0][i] = math.exp(exponent)\n",
        "    pass\n",
        "  return np.dot(A, w)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NjTdc7-IF9Z3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2: Math\n",
        "This is where I start flying completely blind. I'm going to lay out my process and equations and hope for the best.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ij8ORsC4GiDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Finding $E_{in}$\n",
        "\n",
        "$E_{in}$ stands for the in-sample error.\n",
        "\n",
        "This is actually exactly the same as the previous loss function. We calculated the mean squared error back then, and since we still want that same thing we're going to do it again here. I'm going to write it down anyways just to make sure all the pieces are down here and not wayy up there.\n",
        "\n",
        "\\begin{equation}\n",
        "E_{in} = \\| y - f(\\mathbf{x})\\|^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Notice that even though the code is identical, ```precision_rate``` is now actually a vector. It doesn't matter here because it was just passed straight through, into the predict function, which is now called ```forward```\n"
      ]
    },
    {
      "metadata": {
        "id": "8I9JlbT0GlRm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def refined_loss(X, Y, precision_rate, mu, w):\n",
        "  mse = 0\n",
        "  count = 0\n",
        "  for i in range(X.shape[0]):\n",
        "    predicted = forward(mu, w, X[i], precision_rate)\n",
        "    err = Y[i] - predicted\n",
        "    mse += err * err\n",
        "    count += 1\n",
        "    pass\n",
        "  res = mse / count\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pb_Ga_IxLoUR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Regression on $\\mathbf{w}$\n",
        "\n",
        "Funnily enough, this function is actually just going to be the previous train function. Given a set of $\\lambda$s, we already know how to solve for $\\mathbf{w}$. Linear regression gives us the best possible answer to this problem."
      ]
    },
    {
      "metadata": {
        "id": "r_RvJiyGNfnG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def regress_w (X, Y, precision_rate, mu):\n",
        "  A = np.zeros((X.shape[0], mu.shape[0]))\n",
        "#   iterate over X\n",
        "  for i in range(X.shape[0]):\n",
        "#     iterate over mu\n",
        "    for j in range(mu.shape[0]):\n",
        "#     create vectors for x and mu\n",
        "      _x = np.transpose([X[i]])\n",
        "      _mu = np.transpose([mu[j]])\n",
        "#     Take the distance between point x and mu\n",
        "      dist = np.linalg.norm(_x - _mu)\n",
        "  #   square distance, multiply by lambda\n",
        "      exponent = (- precision_rate[j]) * dist * dist\n",
        "      A[i][j] = math.exp(exponent)\n",
        "      pass\n",
        "    pass\n",
        "#   print(precision_rate)\n",
        "  \n",
        "#   Get ATA\n",
        "  transpose = np.transpose(A)\n",
        "  ATA = np.dot(transpose, A)\n",
        "  \n",
        "#   Invert ATA\n",
        "  pseudoInv = np.linalg.inv(ATA)\n",
        "  \n",
        "#   Take(ATA)^-1 ATy\n",
        "  res = np.dot(transpose, Y)\n",
        "  res = np.dot(pseudoInv, res)\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YjdrlsjaHRX_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Derivative on $\\lambda$\n",
        "\n",
        "This is just math here, but what I want from this is a way to find the effect each $\\lambda\n",
        "$ has on my final $E_{in}$. So to do that I'm going to find the derivative for each lambda.\n",
        "\n",
        "\\begin{equation}\n",
        "E_{in} = \\| y - f(\\mathbf{x})\\|^2\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "f(\\mathbf{x}) = \\sum_{k=1}^K w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\frac{\\partial E_{in}}{\\partial \\lambda_k} = \\frac{\\partial E_{in}}{\\partial F} \\cdot \\frac{\\partial F}{\\partial \\lambda_k}\n",
        "\\end{equation}\n",
        "Since\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial E_{in}}{\\partial F} = 2\\cdot (y - f(\\mathbf{x}))\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial F}{\\partial \\lambda_k} = - \\mathbf{\\| x - \\mu_k\\|^2} w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n",
        "\\end{equation}\n",
        "\n",
        "We get the following:\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\frac{\\partial E_{in}}{\\partial \\lambda_k} = - 2 \\dot (y - f(\\mathbf{x})) \\mathbf{\\| x - \\mu_k\\|^2} w_k e^{-\\lambda_k \\mathbf{\\| x - \\mu_k \\|^2}}\n",
        "\\end{equation}\n",
        "\n",
        "Now, this looks fucking ridiculous. I think there's a 99% chance I got this all wrong, but since $\\lambda$ contribiutes nonlinear way to the model, I'm going to just move forward and see if it works out. I will be unbelievably surprised and smug if it does.\n",
        "\n",
        "Anyways, as a result of this equation, if we can consider each $\\lambda_k$ to come together and form a vector, namely $\\vec{\\lambda}$, I'm going to write the full gradient of $F$ according to this vector.\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\frac{\\partial E_{in}}{\\partial \\vec{\\lambda}} = \n",
        "- 2 \\dot (y - f(\\mathbf{x}))\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{\\| x - \\mu_1\\|^2} w_1 e^{-\\lambda_1 \\mathbf{\\| x - \\mu_1 \\|^2}} \\\\\n",
        "\\mathbf{\\| x - \\mu_2\\|^2} w_2 e^{-\\lambda_2 \\mathbf{\\| x - \\mu_2 \\|^2}} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{\\| x - \\mu_K\\|^2} w_K e^{-\\lambda_K \\mathbf{\\| x - \\mu_K \\|^2}}\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Now this is a fucking monster."
      ]
    },
    {
      "metadata": {
        "id": "Ns6Dr1dY2-MB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stochastic Gradient Descent\n",
        "\n",
        "Now that we have this gradient, what does it mean? well it means that, in theory, if all other things are contants in this equation, that the direction in $\\vec{\\lambda}$ space given by $\\frac{\\partial E_{in}}{\\partial \\vec{\\lambda}}$ should increase $E_{in}$ by the largest amount - that's where its **gradient** is steepest. But that's not what we want! We actually want two things:\n",
        " - We want $E_{in}$ to **decrease**, not increase.\n",
        " - We want $\\vec{\\lambda}$ to change really slowly, or by a tiny amount, each time we apply it for a given $\\mathbf{x}$. We can't have it optimize for each individual datapoint, because then our model will only be optimized for the most recent entry it was trained on. In other words, we want $E_{in}$ to decrease **stochastically**, little by little.\n",
        "\n",
        "So, by putting these ideas together, let's define stochastic gradient descent as follows:\n",
        " - Pick a learning rate, or $\\eta$, that is very small, like between $0$ and $0.1$.\n",
        " - Start with a random $\\vec{\\lambda}$, and update it for every $\\mathbf{x}$ as follows:\n",
        " \n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\vec{\\lambda} = \\vec{\\lambda} - \\frac{\\partial E_{in}}{\\partial \\vec{\\lambda}}\n",
        "\\end{equation}\n",
        " \n",
        " So, to properly perform gradient descent for $\\vec{\\lambda}$, all we have to do is define a fucntion that updates $\\vec{\\lambda}$ as follows:\n",
        " \n",
        " \n",
        " \\begin{equation}\n",
        "\\Large\n",
        "\\vec{\\lambda} = \\vec{\\lambda}\n",
        "+\n",
        "\\eta\n",
        "\\cdot\n",
        " 2 \\dot (y - f(\\mathbf{x}))\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{\\| x - \\mu_1\\|^2} w_1 e^{-\\lambda_1 \\mathbf{\\| x - \\mu_1 \\|^2}} \\\\\n",
        "\\mathbf{\\| x - \\mu_2\\|^2} w_2 e^{-\\lambda_2 \\mathbf{\\| x - \\mu_2 \\|^2}} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{\\| x - \\mu_K\\|^2} w_K e^{-\\lambda_K \\mathbf{\\| x - \\mu_K \\|^2}}\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "UH4TPBhx2bbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def descend_lambda (X, Y, precision_rate, learning_rate, mu, w):\n",
        "  new_lambda = precision_rate\n",
        "#   loop over X\n",
        "  for i in range(X.shape[0]):\n",
        "#     calculate f partial\n",
        "    f_partial = np.zeros((mu.shape[0], 1))\n",
        "#   Loop over mu\n",
        "    for j in range(mu.shape[0]):\n",
        "#     get distance between x and mu\n",
        "      dist = np.linalg.norm(X[i] - mu[j])\n",
        "      exponent = (- precision_rate[j]) * dist * dist\n",
        "#     raise e to -lambda times squared distance\n",
        "      e_to_exponent = math.exp(exponent)\n",
        "#     assign squared distance times exponential to partial\n",
        "      f_partial[j][0] = dist * dist * e_to_exponent\n",
        "      pass\n",
        "#   Calculate E partial\n",
        "    err = Y[i] - forward(mu, w, X[i], precision_rate)\n",
        "    e_partial = 2 * err\n",
        "#     combine partials with learning rate eta\n",
        "    final_partial = learning_rate * e_partial * f_partial\n",
        "#   Increment lambda by final partial\n",
        "    new_lambda = precision_rate + final_partial\n",
        "    pass\n",
        "  return new_lambda\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEfzlsQFAyv5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3: Training\n",
        "\n",
        "I feel like I cannot express enough times how sure I am that this is going to fail miserably. I'm going to just finish this off and witness my ridiculous failure in terms of MSE, and then slowly fine tune it\n",
        "\n",
        "Now, I technically have  2  ```train``` functions that optimize our two sets of variable over a number of epochs. During these epochs I'm going to:\n",
        " - Perform linear regression on $\\mathbf{w}$\n",
        " - Perform gradient descent on $\\vec{\\lambda}$\n",
        " \n",
        "Notice that regression doesn't take in a learning rate, it actually gives me the **best possible $\\mathbf{w}$** given a $\\vec{\\lambda}$. Gradient descent slowly improves on  $\\vec{\\lambda}$ over the course of one epoch (an epoch is a run-through of all datapoints), but at the beginning of each epoch $\\vec{\\lambda}$ will get a whole new $\\mathbf{w}$ to descend upon."
      ]
    },
    {
      "metadata": {
        "id": "1RHS2PfxDykM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "a180aa98-2a5d-4bc1-ebba-13cd0727a436"
      },
      "cell_type": "code",
      "source": [
        "X = train_set.drop(target, axis=1).values\n",
        "Y = train_set[[target]].values\n",
        "\n",
        "test_X = test_set.drop(target, axis=1).values\n",
        "test_Y = test_set[[target]].values\n",
        "\n",
        "w = np.random.ranf((mu.shape[0],1))\n",
        "precision_rate = np.random.ranf((mu.shape[0], 1)) /10\n",
        "\n",
        "num_epochs = 20\n",
        "learning_rate = 0.00006\n",
        "E_in = []\n",
        "E_out = []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  w = regress_w(X, Y, precision_rate, mu)\n",
        "  precision_rate = descend_lambda(X, Y, precision_rate, learning_rate, mu, w)\n",
        "  in_sample_mse = refined_loss(X, Y, precision_rate, mu, w)\n",
        "  mse = refined_loss(test_X, test_Y, precision_rate, mu, w)\n",
        "  E_in.append(float(in_sample_mse))\n",
        "  E_out.append(float(mse))\n",
        "  print('In Sample is {} and Out of sample is {}'.format(in_sample_mse, mse))\n",
        "  pass\n",
        "print('Final MSE was {}'.format(mse))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(111)\n",
        "plt.plot(np.arange(0,i+1,1), E_in, label='E_in')\n",
        "plt.plot(np.arange(0,i+1,1), E_out, label='E_out')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In Sample is [[9.5711096e+09]] and Out of sample is [[9.12316052e+09]]\n",
            "In Sample is [[30590.7224323]] and Out of sample is [[150993.67327737]]\n",
            "In Sample is [[27901.5433603]] and Out of sample is [[170338.24662698]]\n",
            "In Sample is [[6723.64109079]] and Out of sample is [[27905.64918027]]\n",
            "In Sample is [[2256.5083729]] and Out of sample is [[18500.99489815]]\n",
            "In Sample is [[1645.55288298]] and Out of sample is [[22088.64520734]]\n",
            "In Sample is [[1167.50096154]] and Out of sample is [[23772.15855721]]\n",
            "In Sample is [[988.06120204]] and Out of sample is [[24943.21970215]]\n",
            "In Sample is [[888.77822849]] and Out of sample is [[25805.56970866]]\n",
            "In Sample is [[820.05544862]] and Out of sample is [[26464.9751121]]\n",
            "In Sample is [[768.57107516]] and Out of sample is [[26987.49885318]]\n",
            "In Sample is [[728.95209159]] and Out of sample is [[27414.70665944]]\n",
            "In Sample is [[698.11235931]] and Out of sample is [[27773.27609264]]\n",
            "In Sample is [[673.93317233]] and Out of sample is [[28080.81311359]]\n",
            "In Sample is [[654.86127523]] and Out of sample is [[28349.28561246]]\n",
            "In Sample is [[639.73208177]] and Out of sample is [[28587.06685699]]\n",
            "In Sample is [[627.6643352]] and Out of sample is [[28800.17743825]]\n",
            "In Sample is [[617.98724905]] and Out of sample is [[28993.05900707]]\n",
            "In Sample is [[610.1874644]] and Out of sample is [[29169.06828697]]\n",
            "In Sample is [[603.86988024]] and Out of sample is [[29330.79987987]]\n",
            "Final MSE was [[29330.79987987]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFVCAYAAADYEVdtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCXgz4CFIp4kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4fe4d32190>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pOWgWmKCN8U-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4: Review"
      ]
    }
  ]
}